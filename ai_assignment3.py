# -*- coding: utf-8 -*-
"""AI_Assignment3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12owPePMXuwU6eT-Wfv4OJKUe0S-KRmcb
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout, concatenate
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import GridSearchCV, train_test_split
from keras.models import Sequential
from keras.layers import Dense
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, roc_auc_score

df = pd.read_csv('/content/drive/MyDrive/Classroom/CustomerChurn_dataset.csv')

df = df.drop('customerID', axis=1)

df.info()

categorical_columns = df.select_dtypes(include=['object']).columns.tolist()

num_columns = df.select_dtypes(include=['float64','int64']).columns.tolist()

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Apply LabelEncoder to each categorical column
for column in categorical_columns:
    df[column] = label_encoder.fit_transform(df[column])

df.info()

"""Feature Extraction"""

X =df.drop('Churn', axis=1)
y = df['Churn']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

rf_classifier.fit(X_train, y_train)

feature_importances = rf_classifier.feature_importances_

feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})

# Sort the DataFrame by importance in descending order
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Extract the top 5 important features
top_5_features = feature_importance_df.head(5)

print(top_5_features)

"""Exploratory Data **Analysis**"""

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming df is your DataFrame
plt.figure(figsize=(10, 6))
sns.barplot(x='Churn', y='MonthlyCharges', data=df, ci=None)
plt.title('Average Monthly Charges for Churned and Non-Churned Customers')
plt.xlabel('Churn')
plt.ylabel('Average Monthly Charges')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming df is your DataFrame
plt.figure(figsize=(12, 6))
sns.boxplot(x='Churn', y='tenure', data=df)
plt.title('Distribution of Tenure for Churned and Non-Churned Customers')
plt.xlabel('Churn')
plt.ylabel('Tenure')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming df is your DataFrame
plt.figure(figsize=(12, 6))
sns.boxplot(x='Churn', y='TotalCharges', data=df)
plt.title('Distribution of TotalCharges for Churned and Non-Churned Customers')
plt.xlabel('Churn')
plt.ylabel('TotalCharges')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming df is your DataFrame
plt.figure(figsize=(10, 6))
sns.countplot(x='Contract', hue='Churn', data=df)
plt.title('Churn Count by Contract Type')
plt.xlabel('Contract Type')
plt.ylabel('Count')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming df is your DataFrame
plt.figure(figsize=(12, 6))
sns.countplot(x='PaymentMethod', hue='Churn', data=df)
plt.title('Churn Count by Payment Method')
plt.xlabel('Payment Method')
plt.ylabel('Count')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability
plt.show()

# Assuming df is your DataFrame with the relevant features and target 'Churn'
selected_features = ['MonthlyCharges', 'tenure', 'TotalCharges', 'Contract', 'PaymentMethod']
X = df[selected_features]
y = df['Churn']

# Preprocess categorical features using LabelEncoder
label_encoder = LabelEncoder()
X['Contract'] = label_encoder.fit_transform(X['Contract'])
X['PaymentMethod'] = label_encoder.fit_transform(X['PaymentMethod'])

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize numerical features
numeric_features = ['MonthlyCharges', 'tenure', 'TotalCharges']
scaler = StandardScaler()

X_train_numeric = scaler.fit_transform(X_train[numeric_features])
X_test_numeric = scaler.transform(X_test[numeric_features])

X_train_categorical = X_train[['Contract', 'PaymentMethod']].values
X_test_categorical = X_test[['Contract', 'PaymentMethod']].values

# Concatenate the preprocessed features
X_train_processed = np.concatenate([X_train_numeric, X_train_categorical], axis=1)
X_test_processed = np.concatenate([X_test_numeric, X_test_categorical], axis=1)

# Build the MLP model using Functional API
input_layer = Input(shape=(X_train_processed.shape[1],))
dense_1 = Dense(64, activation='relu')(input_layer)
dropout_1 = Dropout(0.5)(dense_1)
dense_2 = Dense(32, activation='relu')(dropout_1)
output_layer = Dense(1, activation='sigmoid')(dense_2)

model = Model(inputs=input_layer, outputs=output_layer)

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train_processed, y_train, epochs=10, batch_size=32, validation_data=(X_test,y_test))
#split=0.2, verbose=2
# Evaluate the model on the test set
y_pred = (model.predict(X_test_processed) > 0.5).astype(int)
accuracy = accuracy_score(y_test, y_pred)

print(f'Test Accuracy: {accuracy * 100:.2f}%')

import numpy as np
from sklearn.model_selection import GridSearchCV
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.neural_network import MLPClassifier


# Function to create Keras model
def create_model(optimizer='adam', dropout_rate=0.5):
    input_layer = Input(shape=(X_train_processed.shape[1],))
    dense_1 = Dense(64, activation='relu')(input_layer)
    dropout_1 = Dropout(dropout_rate)(dense_1)
    dense_2 = Dense(32, activation='relu')(dropout_1)
    output_layer = Dense(1, activation='sigmoid')(dense_2)

    model = Model(inputs=input_layer, outputs=output_layer)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

    return model


# Define the parameter grid to search over
param_grid = {
    'hidden_layer_sizes': [(32,), (64,), (32, 32), (64, 32)],
    'activation': ['relu', 'tanh', 'logistic'],
    'alpha': [0.0001, 0.001, 0.01],
    'learning_rate': ['constant', 'invscaling', 'adaptive'],
}

# Create the MLPClassifier
mlp = MLPClassifier(max_iter=100)

# Create the GridSearchCV object
grid_search = GridSearchCV(mlp, param_grid, cv=3, scoring='accuracy', n_jobs=-1)

# Fit the grid search to the data
grid_search.fit(X_train_processed, y_train)

# Get the best parameters and best model
best_parameters = grid_search.best_params_
best_model = grid_search.best_estimator_

# Evaluate the best model on the test set
test_accuracy = best_model.score(X_test_processed, y_test)

print("Best Parameters:", best_parameters)
print("Test Accuracy:", test_accuracy)

#creating the mlp using the beset parameters from the grid search
import tensorflow as tf
input_layer = Input(shape=(X_train_processed.shape[1],))
dense_1 = Dense(64, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(0.01))(input_layer)
dropout_1 = Dropout(0.01)(dense_1)
dense_2 = Dense(32, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(0.01))(dropout_1)
output_layer = Dense(1, activation='sigmoid')(dense_2)

model = Model(inputs=input_layer, outputs=output_layer)

# Compile the model
optimizer = Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train_processed, y_train, epochs=10, batch_size=32, validation_data=(X_test_processed, y_test), verbose=2)

# Evaluate the model on the test set
y_pred = (model.predict(X_test_processed) > 0.5).astype(int)
accuracy = accuracy_score(y_test, y_pred)

print(f'Test Accuracy: {accuracy * 100:.2f}%')

y_pred_initial = (model.predict(X_test_processed) > 0.5).astype(int)

accuracy_initial = accuracy_score(y_test, y_pred_initial)
auc_initial = roc_auc_score(y_test, model.predict(X_test_processed))

print(f'Initial model - Test Accuracy: {accuracy_initial * 100:.2f}%')
print(f'Initial model - AUC Score: {auc_initial:.4f}')

# Save the model to a file
model.save("Churning_Customers.h5")